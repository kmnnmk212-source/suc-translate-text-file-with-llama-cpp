{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpf-dLdxoxbg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_JRSqepyWQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJIcGJDlyWTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/mradermacher/Riva-Translate-4B-Instruct-i1-GGUF/resolve/main/Riva-Translate-4B-Instruct.i1-Q6_K.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezcs4pKUyWZ_",
        "outputId": "609f26ab-59c7-4cd2-a7b0-cf75349347a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-15 20:52:45--  https://huggingface.co/mradermacher/Riva-Translate-4B-Instruct-i1-GGUF/resolve/main/Riva-Translate-4B-Instruct.i1-Q6_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 108.157.142.53, 108.157.142.74, 108.157.142.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.157.142.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6873e0cba6723e94d8198ce5/0d704b0e37a58fe56acc2199a49a1e82747dbfb188e0c5d6a80b80566f3da8c1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251215T205245Z&X-Amz-Expires=3600&X-Amz-Signature=2fbeb8fa943b6630b0ff902b4bacaa883eaf79733a8736d35005bfec22920d08&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Riva-Translate-4B-Instruct.i1-Q6_K.gguf%3B+filename%3D%22Riva-Translate-4B-Instruct.i1-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1765835565&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTgzNTU2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODczZTBjYmE2NzIzZTk0ZDgxOThjZTUvMGQ3MDRiMGUzN2E1OGZlNTZhY2MyMTk5YTQ5YTFlODI3NDdkYmZiMTg4ZTBjNWQ2YTgwYjgwNTY2ZjNkYThjMSoifV19&Signature=Yf238EFW7CCc-7tVutVNy9Cocc9mbHzn01tTBQ89jiGFJDJJb%7E646UJ7SFRqjC%7Ee1S2gjXXS0MBwNtS-hVRx3xVNRwf4LCRiw726mbc45q3KCMSYiPIdmFAgSKZv97TD3xe4Wsn5DxNEtIN9wtDN1Z3OYiOCkG6%7EX-H-kPYBSJXTT8tXOWHH01TGjEHoKFH7bfMamQFoYXZtGqUx5CS3TwY2uM1ERgc72geuLR10H2PLhpd9tmTaGnZhFAor4Sd9tOmWHrGsBpiBcT4SytPzMqq%7Ev-wZAv92O1NgiFzlxrva1noWAeyps4%7EKIeOsDdR-N26KDeCMvxT7JsgHXOdiYA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-15 20:52:45--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6873e0cba6723e94d8198ce5/0d704b0e37a58fe56acc2199a49a1e82747dbfb188e0c5d6a80b80566f3da8c1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251215T205245Z&X-Amz-Expires=3600&X-Amz-Signature=2fbeb8fa943b6630b0ff902b4bacaa883eaf79733a8736d35005bfec22920d08&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Riva-Translate-4B-Instruct.i1-Q6_K.gguf%3B+filename%3D%22Riva-Translate-4B-Instruct.i1-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1765835565&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTgzNTU2NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODczZTBjYmE2NzIzZTk0ZDgxOThjZTUvMGQ3MDRiMGUzN2E1OGZlNTZhY2MyMTk5YTQ5YTFlODI3NDdkYmZiMTg4ZTBjNWQ2YTgwYjgwNTY2ZjNkYThjMSoifV19&Signature=Yf238EFW7CCc-7tVutVNy9Cocc9mbHzn01tTBQ89jiGFJDJJb%7E646UJ7SFRqjC%7Ee1S2gjXXS0MBwNtS-hVRx3xVNRwf4LCRiw726mbc45q3KCMSYiPIdmFAgSKZv97TD3xe4Wsn5DxNEtIN9wtDN1Z3OYiOCkG6%7EX-H-kPYBSJXTT8tXOWHH01TGjEHoKFH7bfMamQFoYXZtGqUx5CS3TwY2uM1ERgc72geuLR10H2PLhpd9tmTaGnZhFAor4Sd9tOmWHrGsBpiBcT4SytPzMqq%7Ev-wZAv92O1NgiFzlxrva1noWAeyps4%7EKIeOsDdR-N26KDeCMvxT7JsgHXOdiYA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 54.192.100.121, 54.192.100.7, 54.192.100.74, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|54.192.100.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3655776832 (3.4G)\n",
            "Saving to: ‘Riva-Translate-4B-Instruct.i1-Q6_K.gguf’\n",
            "\n",
            "Riva-Translate-4B-I 100%[===================>]   3.40G  33.6MB/s    in 94s     \n",
            "\n",
            "2025-12-15 20:54:19 (37.0 MB/s) - ‘Riva-Translate-4B-Instruct.i1-Q6_K.gguf’ saved [3655776832/3655776832]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b7411/llama-b7411-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Eutt-u-yXix",
        "outputId": "2e70310c-3132-489c-ede7-6c7eec1267d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-15 20:54:32--  https://github.com/ggml-org/llama.cpp/releases/download/b7411/llama-b7411-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/612354784/43c9651c-d76e-4917-9a50-7ec8cf204af2?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-15T21%3A42%3A27Z&rscd=attachment%3B+filename%3Dllama-b7411-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-15T20%3A41%3A35Z&ske=2025-12-15T21%3A42%3A27Z&sks=b&skv=2018-11-09&sig=LEXnpX8sTaLqraAq8snBIy6gt1nYV%2F7%2F0W1bj8vRKOw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTgzMzg3MiwibmJmIjoxNzY1ODMyMDcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.n4WyPivujnhtEsGIaB6Bfbg85wBldzF2HaXh3hVUKVo&response-content-disposition=attachment%3B%20filename%3Dllama-b7411-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-12-15 20:54:32--  https://release-assets.githubusercontent.com/github-production-release-asset/612354784/43c9651c-d76e-4917-9a50-7ec8cf204af2?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-15T21%3A42%3A27Z&rscd=attachment%3B+filename%3Dllama-b7411-bin-ubuntu-x64.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-15T20%3A41%3A35Z&ske=2025-12-15T21%3A42%3A27Z&sks=b&skv=2018-11-09&sig=LEXnpX8sTaLqraAq8snBIy6gt1nYV%2F7%2F0W1bj8vRKOw%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NTgzMzg3MiwibmJmIjoxNzY1ODMyMDcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.n4WyPivujnhtEsGIaB6Bfbg85wBldzF2HaXh3hVUKVo&response-content-disposition=attachment%3B%20filename%3Dllama-b7411-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18925593 (18M) [application/octet-stream]\n",
            "Saving to: ‘llama-b7411-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b7411-bin-ubu 100%[===================>]  18.05M  91.3MB/s    in 0.2s    \n",
            "\n",
            "2025-12-15 20:54:32 (91.3 MB/s) - ‘llama-b7411-bin-ubuntu-x64.zip’ saved [18925593/18925593]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b7411-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSwsSnR4yxrg",
        "outputId": "4719d0ce-eda2-4e27-8e21-d59941bd7540"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b7411-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE-curl  \n",
            "  inflating: build/bin/LICENSE-httplib  \n",
            "  inflating: build/bin/LICENSE-jsonhpp  \n",
            "  inflating: build/bin/LICENSE-linenoise  \n",
            "    linking: build/bin/libggml-base.so  -> libggml-base.so.0 \n",
            "    linking: build/bin/libggml-base.so.0  -> libggml-base.so.0.9.4 \n",
            "  inflating: build/bin/libggml-base.so.0.9.4  \n",
            "  inflating: build/bin/libggml-cpu-alderlake.so  \n",
            "  inflating: build/bin/libggml-cpu-haswell.so  \n",
            "  inflating: build/bin/libggml-cpu-icelake.so  \n",
            "  inflating: build/bin/libggml-cpu-sandybridge.so  \n",
            "  inflating: build/bin/libggml-cpu-sapphirerapids.so  \n",
            "  inflating: build/bin/libggml-cpu-skylakex.so  \n",
            "  inflating: build/bin/libggml-cpu-sse42.so  \n",
            "  inflating: build/bin/libggml-cpu-x64.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "    linking: build/bin/libggml.so    -> libggml.so.0 \n",
            "    linking: build/bin/libggml.so.0  -> libggml.so.0.9.4 \n",
            "  inflating: build/bin/libggml.so.0.9.4  \n",
            "    linking: build/bin/libllama.so   -> libllama.so.0 \n",
            "    linking: build/bin/libllama.so.0  -> libllama.so.0.0.7411 \n",
            "  inflating: build/bin/libllama.so.0.0.7411  \n",
            "    linking: build/bin/libmtmd.so    -> libmtmd.so.0 \n",
            "    linking: build/bin/libmtmd.so.0  -> libmtmd.so.0.0.7411 \n",
            "  inflating: build/bin/libmtmd.so.0.0.7411  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-completion  \n",
            "  inflating: build/bin/llama-fit-params  \n",
            "  inflating: build/bin/llama-gemma3-cli  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-mtmd-cli  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/rpc-server    \n",
            "finishing deferred symbolic links:\n",
            "  build/bin/libggml-base.so -> libggml-base.so.0\n",
            "  build/bin/libggml-base.so.0 -> libggml-base.so.0.9.4\n",
            "  build/bin/libggml.so   -> libggml.so.0\n",
            "  build/bin/libggml.so.0 -> libggml.so.0.9.4\n",
            "  build/bin/libllama.so  -> libllama.so.0\n",
            "  build/bin/libllama.so.0 -> libllama.so.0.0.7411\n",
            "  build/bin/libmtmd.so   -> libmtmd.so.0\n",
            "  build/bin/libmtmd.so.0 -> libmtmd.so.0.0.7411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKyutNVIyznP",
        "outputId": "bde0508f-be46-4b04-ab77-2a536985fbce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "-cl,   --cache-list                     show list of models in cache\n",
            "--completion-bash                       print source-able bash completion script for llama.cpp\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of CPU threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),\n",
            "                                        realtime(3) (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "--swa-full                              use full-size SWA cache (default: false)\n",
            "                                        [(more\n",
            "                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
            "                                        (env: LLAMA_ARG_SWA_FULL)\n",
            "--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences\n",
            "                                        (default: false)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)\n",
            "                                        (env: LLAMA_ARG_KV_UNIFIED)\n",
            "-fa,   --flash-attn [on|off|auto]       set Flash Attention use ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with; for system message, use -sys\n",
            "--perf, --no-perf                       whether to enable internal libllama performance timings (default:\n",
            "                                        false)\n",
            "                                        (env: LLAMA_ARG_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape, --no-escape            whether to process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n",
            "                                        (default: true)\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: -1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-kvo,  --kv-offload, -nkvo, --no-kv-offload\n",
            "                                        whether to enable KV cache offloading (default: enabled)\n",
            "                                        (env: LLAMA_ARG_KV_OFFLOAD)\n",
            "--repack, -nr, --no-repack              whether to enable weight repacking (default: enabled)\n",
            "                                        (env: LLAMA_ARG_REPACK)\n",
            "--no-host                               bypass host buffer allowing extra buffers to be used\n",
            "                                        (env: LLAMA_ARG_NO_HOST)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--mmap, --no-mmap                       whether to memory-map model (if disabled, slower load but may reduce\n",
            "                                        pageouts if not using mlock) (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggml-org/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't\n",
            "                                        offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "                                        (env: LLAMA_ARG_DEVICE)\n",
            "--list-devices                          print list of available devices and exit\n",
            "--override-tensor, -ot <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type\n",
            "--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU\n",
            "                                        (env: LLAMA_ARG_CPU_MOE)\n",
            "--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   max. number of layers to store in VRAM (default: -1)\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "-fit,  --fit [on|off]                   whether to adjust unset arguments to fit in device memory ('on' or\n",
            "                                        'off', default: 'on')\n",
            "                                        (env: LLAMA_ARG_FIT)\n",
            "-fitt, --fit-target MiB                 target margin per device for --fit option, default: 1024\n",
            "                                        (env: LLAMA_ARG_FIT_TARGET)\n",
            "-fitc, --fit-ctx N                      minimum ctx size that can be set by --fit option, default: 4096\n",
            "                                        (env: LLAMA_ARG_FIT_CTX)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--op-offload, --no-op-offload           whether to offload host tensor operations to device (default: true)\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path to load\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-dr,   --docker-repo [<repo>/]<model>[:quant]\n",
            "                                        Docker Hub model repository. repo is optional, default to ai/. quant\n",
            "                                        is optional, default to :latest.\n",
            "                                        example: gemma3\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_DOCKER_REPO)\n",
            "-hf,   -hfr, --hf-repo <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository; quant is optional, case-insensitive,\n",
            "                                        default to Q4_K_M, or falls back to the first file in the repo if\n",
            "                                        Q4_K_M doesn't exist.\n",
            "                                        mmproj is also downloaded automatically if available. to disable, add\n",
            "                                        --no-mmproj\n",
            "                                        example: unsloth/phi-4-GGUF:q4_k_m\n",
            "                                        (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]\n",
            "                                        Same as --hf-repo, but for the draft model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HFD_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in\n",
            "                                        --hf-repo (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]\n",
            "                                        Hugging Face model repository for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO_V)\n",
            "-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE_V)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "                                        (env: LLAMA_LOG_FILE)\n",
            "--log-colors [on|off|auto]              Set colored logging ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "--offline                               Offline mode: forces use of cache, prevents network access\n",
            "                                        (env: LLAMA_OFFLINE)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored. Values:\n",
            "                                         - 0: generic output\n",
            "                                         - 1: error\n",
            "                                         - 2: warning\n",
            "                                         - 3: info\n",
            "                                         - 4: debug\n",
            "                                        (default: 1)\n",
            "                                        \n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefix in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)\n",
            "-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model\n",
            "                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1\n",
            "                                        (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default:\n",
            "                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq, --sampler-seq SEQUENCE\n",
            "                                        simplified sequence for samplers that will be used (default:\n",
            "                                        edskypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "                                        (env: LLAMA_ARG_TOP_K)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations\n",
            "                                        (https://json-schema.org/), e.g. `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--display-prompt, --no-display-prompt   whether to print prompt at generation (default: true)\n",
            "-co,   --color [on|off|auto]            Colorize output to distinguish prompt and user input from generations\n",
            "                                        ('on', 'off', or 'auto', default: 'auto')\n",
            "                                        'auto' enables colors when output is to a terminal\n",
            "--ctx-checkpoints, --swa-checkpoints N\n",
            "                                        max number of context checkpoints to create per slot (default: 8)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)\n",
            "                                        (env: LLAMA_ARG_CTX_CHECKPOINTS)\n",
            "--cache-ram, -cram N                    set the maximum cache size in MiB (default: 8192, -1 - no limit, 0 -\n",
            "                                        disable)\n",
            "                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/16391)\n",
            "                                        (env: LLAMA_ARG_CACHE_RAM)\n",
            "--context-shift, --no-context-shift     whether to use context shift on infinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_CONTEXT_SHIFT)\n",
            "-sys,  --system-prompt PROMPT           system prompt to use with model (if applicable, depending on chat\n",
            "                                        template)\n",
            "--show-timings, --no-show-timings       whether to show timing information after each response (default: true)\n",
            "                                        (env: LLAMA_ARG_SHOW_TIMINGS)\n",
            "-sysf, --system-prompt-file FNAME       a file containing the system prompt (default: none)\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation, -no-cnv, --no-conversation\n",
            "                                        whether to run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: auto enabled if chat template is available)\n",
            "-st,   --single-turn                    run conversation for a single turn only, then exit when done\n",
            "                                        will not be interactive if first turn is predefined with --prompt\n",
            "                                        (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--warmup, --no-warmup                   whether to perform warmup with an empty run (default: enabled)\n",
            "-mm,   --mmproj FILE                    path to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        note: if -hf is used, this argument can be omitted\n",
            "                                        (env: LLAMA_ARG_MMPROJ)\n",
            "-mmu,  --mmproj-url URL                 URL to a multimodal projector file. see tools/mtmd/README.md\n",
            "                                        (env: LLAMA_ARG_MMPROJ_URL)\n",
            "--mmproj-auto, --no-mmproj, --no-mmproj-auto\n",
            "                                        whether to use multimodal projector file (if available), useful when\n",
            "                                        using -hf (default: enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_AUTO)\n",
            "--mmproj-offload, --no-mmproj-offload   whether to enable GPU offloading for multimodal projector (default:\n",
            "                                        enabled)\n",
            "                                        (env: LLAMA_ARG_MMPROJ_OFFLOAD)\n",
            "--image, --audio FILE                   path to an image or audio file. use with multimodal models, can be\n",
            "                                        repeated if you have multiple files\n",
            "--image-min-tokens N                    minimum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MIN_TOKENS)\n",
            "--image-max-tokens N                    maximum number of tokens each image can take, only used by vision\n",
            "                                        models with dynamic resolution (default: read from model)\n",
            "                                        (env: LLAMA_ARG_IMAGE_MAX_TOKENS)\n",
            "--override-tensor-draft, -otd <tensor name pattern>=<buffer type>,...\n",
            "                                        override tensor buffer type for draft model\n",
            "--cpu-moe-draft, -cmoed                 keep all Mixture of Experts (MoE) weights in the CPU for the draft\n",
            "                                        model\n",
            "                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)\n",
            "--n-cpu-moe-draft, -ncmoed N            keep the Mixture of Experts (MoE) weights of the first N layers in the\n",
            "                                        CPU for the draft model\n",
            "                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)\n",
            "--chat-template-kwargs STRING           sets additional params for the json template parser\n",
            "                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)\n",
            "--jinja, --no-jinja                     whether to use jinja template engine for chat (default: enabled)\n",
            "                                        (env: LLAMA_ARG_JINJA)\n",
            "--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the\n",
            "                                        response, and in which format they're returned; one of:\n",
            "                                        - none: leaves thoughts unparsed in `message.content`\n",
            "                                        - deepseek: puts thoughts in `message.reasoning_content`\n",
            "                                        - deepseek-legacy: keeps `<think>` tags in `message.content` while\n",
            "                                        also populating `message.reasoning_content`\n",
            "                                        (default: auto)\n",
            "                                        (env: LLAMA_ARG_THINK)\n",
            "--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for\n",
            "                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)\n",
            "                                        (env: LLAMA_ARG_THINK_BUDGET)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--chat-template-file JINJA_TEMPLATE_FILE\n",
            "                                        set custom jinja chat template file (default: template taken from\n",
            "                                        model's metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted (unless --jinja is set\n",
            "                                        before this flag):\n",
            "                                        list of built-in templates:\n",
            "                                        bailing, bailing-think, bailing2, chatglm3, chatglm4, chatml,\n",
            "                                        command-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3,\n",
            "                                        gemma, gigachat, glmedge, gpt-oss, granite, grok-2, hunyuan-dense,\n",
            "                                        hunyuan-moe, kimi-k2, llama2, llama2-sys, llama2-sys-bos,\n",
            "                                        llama2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1,\n",
            "                                        mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch,\n",
            "                                        openchat, orion, pangu-embedded, phi3, phi4, rwkv-world, seed_oss,\n",
            "                                        smolvlm, vicuna, vicuna-orca, yandex, zephyr\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "--draft-max, --draft, --draft-n N       number of tokens to draft for speculative decoding (default: 16)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MAX)\n",
            "--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding\n",
            "                                        (default: 0)\n",
            "                                        (env: LLAMA_ARG_DRAFT_MIN)\n",
            "--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)\n",
            "                                        (env: LLAMA_ARG_DRAFT_P_MIN)\n",
            "-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded\n",
            "                                        from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)\n",
            "-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model\n",
            "                                        (none = don't offload)\n",
            "                                        use --list-devices to see a list of available devices\n",
            "-ngld, --gpu-layers-draft, --n-gpu-layers-draft N\n",
            "                                        number of layers to store in VRAM for the draft model\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)\n",
            "-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_DRAFT)\n",
            "--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main\n",
            "                                        model are not compatible\n",
            "--gpt-oss-20b-default                   use gpt-oss-20b (note: can download weights from the internet)\n",
            "--gpt-oss-120b-default                  use gpt-oss-120b (note: can download weights from the internet)\n",
            "--vision-gemma-4b-default               use Gemma 3 4B QAT (note: can download weights from the internet)\n",
            "--vision-gemma-12b-default              use Gemma 3 12B QAT (note: can download weights from the internet)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Riva-Translate-4B-Instruct.i1-Q6_K.gguf -p \"translate into arabic welcome\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRKzKXY4y473",
        "outputId": "8c76a098-0aa3-481c-808d-3101cd857562"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7411-165caaf5f\n",
            "model      : Riva-Translate-4B-Instruct.i1-Q6_K.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> translate into arabic welcome\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b \bWelcome\n",
            "\u001b[35m\n",
            "[ Prompt: 2.7 t/s | Generation: 3.5 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> \u001b[0m\n",
            "/content/build/bin/libggml-base.so.0(+0x1844b)[0x7a8a862ad44b]\n",
            "/content/build/bin/libggml-base.so.0(ggml_print_backtrace+0x21f)[0x7a8a862ad8af]\n",
            "/content/build/bin/libggml-base.so.0(+0x2bbdf)[0x7a8a862c0bdf]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xae20c)[0x7a8a8611520c]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(+0xae277)[0x7a8a86115277]\n",
            "./build/bin/llama-cli(+0x26b801)[0x58be5f2f7801]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x45495)[0x7a8a85d7c495]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7a8a85d7c610]\n",
            "./build/bin/llama-cli(+0x6f53f)[0x58be5f0fb53f]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7a8a85d79520]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x91117)[0x7a8a85dc8117]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x96624)[0x7a8a85dcd624]\n",
            "/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt6thread4joinEv+0x17)[0x7a8a861432c7]\n",
            "./build/bin/llama-cli(+0x26ad27)[0x58be5f2f6d27]\n",
            "./build/bin/llama-cli(+0x22e33d)[0x58be5f2ba33d]\n",
            "./build/bin/llama-cli(+0x6c73a)[0x58be5f0f873a]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7a8a85d60d90]\n",
            "/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7a8a85d60e40]\n",
            "./build/bin/llama-cli(+0x6f3b5)[0x58be5f0fb3b5]\n",
            "terminate called without an active exception\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Riva-Translate-4B-Instruct.i1-Q6_K.gguf -p \"### Instruction:\\nTranslate the word 'Welcome' into Arabic.\\n\\n### Response:\" -n 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Qc_ZdHEzMS3",
        "outputId": "9cbcba72-d889-485a-d02a-61c72a392ea0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7411-165caaf5f\n",
            "model      : Riva-Translate-4B-Instruct.i1-Q6_K.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            "> ### Instruction:\n",
            "Translate the word 'Welcome' into Arabic.\n",
            "\n",
            "### Response:\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b \b### Instruction:\n",
            "أهلا بك\n",
            "\u001b[35m\n",
            "[ Prompt: 3.1 t/s | Generation: 1.7 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> Translate the phrase \"This commit restore the previous behavior and add an option to force its activation.\" into Arabic.\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \b\"تعيد هذه المشاركة إحياء السلوك السابق وتضيف خيارًا لإجبار تفعيله.\"\n",
            "\u001b[35m\n",
            "[ Prompt: 2.7 t/s | Generation: 1.7 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> /\b/\b/exit\n",
            "\u001b[0m\n",
            "\n",
            "Exiting...\n",
            "llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\n",
            "llama_memory_breakdown_print: |   - Host               |                 4834 =  3478 +    1088 +     268                |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Translate the phrase \"This commit restore the previous behavior and add an option to force its activation.\" into Arabic."
      ],
      "metadata": {
        "id": "_WJWATmJ0BbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Riva-Translate-4B-Instruct.i1-Q6_K.gguf -p \"English: Welcome\\nArabic:\" -n 20"
      ],
      "metadata": {
        "id": "zW2Xj8cFznCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "لترجمة نص طويل باستخدام llama-cli، لا ينصح بوضع النص داخل الأمر مباشرة (داخل علامة التنصيص \" \") لأن سطر الأوامر (Terminal) له حدود، كما أن النص الطويل يحتاج لإعدادات خاصة لضمان عدم انقطاع الترجمة.\n",
        "\n",
        "إليك أفضل طريقة لترجمة النصوص الطويلة:\n",
        "1. تجهيز ملف التوجيه (Prompt File)\n",
        "\n",
        "بدلاً من كتابة النص في التيرمينال، قم بإنشاء ملف نصي جديد (مثلاً سمِّه translate_request.txt) وضع فيه ما يلي:\n",
        "code Text\n",
        "\n",
        "    \n",
        "### Instruction:\n",
        "Translate the following English text into Arabic accurately.\n",
        "\n",
        "### Input:\n",
        "(الصق النص الإنجليزي الطويل هنا...)\n",
        "\n",
        "### Response:\n",
        "\n",
        "  \n",
        "\n",
        "2. تشغيل الأمر مع الإعدادات الصحيحة\n",
        "\n",
        "عليك استخدام \"رايات\" (Flags) إضافية لضمان أن الموديل يستوعب النص الطويل ويكمل الترجمة للنهاية:\n",
        "\n",
        "    -f: لقراءة الملف بدلاً من كتابة النص.\n",
        "\n",
        "    -c: (Context) لزيادة مساحة الذاكرة المؤقتة للنص (مثلاً 4096).\n",
        "\n",
        "    -n: عدد الكلمات التي يولدها الموديل (استخدم -1 لتركه يكمل حتى النهاية).\n",
        "\n",
        "الأمر المقترح:\n",
        "code Bash\n",
        "\n",
        "    \n",
        "!./build/bin/llama-cli -m /content/Riva-Translate-4B-Instruct.i1-Q6_K.gguf -f translate_request.txt -c 4096 -n -1\n",
        "\n",
        "  \n",
        "\n",
        "شرح الإعدادات:\n",
        "\n",
        "    -f translate_request.txt: يقرأ التوجيه والنص من الملف الذي أنشأته، مما يسهل التعامل مع الفقرات الكبيرة.\n",
        "\n",
        "    -c 4096: هذا الرقم مهم جداً. افتراضياً قد يكون السياق 512 فقط. للنصوص الطويلة، يجب رفعه (2048 أو 4096) ليستوعب النص الأصلي + الترجمة.\n",
        "\n",
        "    -n -1: تعني \"لا تتوقف عن الكتابة حتى تنتهي الترجمة\". إذا لم تضع هذا، قد تتوقف الترجمة في منتصف الجملة.\n",
        "\n",
        "ملاحظة هامة للنصوص \"الضخمة جداً\":\n",
        "\n",
        "إذا كان النص عبارة عن صفحات كثيرة (مثلاً كتاب كامل)، فإن الموديل لن يستوعبه دفعة واحدة حتى مع رفع الـ -c. في هذه الحالة، يجب عليك تقسيم النص إلى فقرات (Chunks) وترجمة كل فقرة في ملف منفصل أو طلب واحد تلو الآخر.\n",
        "User"
      ],
      "metadata": {
        "id": "zaFHAxGA34O6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/llama-cli -m /content/Riva-Translate-4B-Instruct.i1-Q6_K.gguf -f translate_request.txt -c 4096 -n -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw_6CL0A35II",
        "outputId": "9bf1dc48-9c9b-40b6-ebe6-dbd8a4c93797"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load_backend: loaded RPC backend from /content/build/bin/libggml-rpc.so\n",
            "load_backend: loaded CPU backend from /content/build/bin/libggml-cpu-haswell.so\n",
            "\n",
            "Loading model... |\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b \b\n",
            "\n",
            "\n",
            "▄▄ ▄▄\n",
            "██ ██\n",
            "██ ██  ▀▀█▄ ███▄███▄  ▀▀█▄    ▄████ ████▄ ████▄\n",
            "██ ██ ▄█▀██ ██ ██ ██ ▄█▀██    ██    ██ ██ ██ ██\n",
            "██ ██ ▀█▄██ ██ ██ ██ ▀█▄██ ██ ▀████ ████▀ ████▀\n",
            "                                    ██    ██\n",
            "                                    ▀▀    ▀▀\n",
            "\n",
            "build      : b7411-165caaf5f\n",
            "model      : Riva-Translate-4B-Instruct.i1-Q6_K.gguf\n",
            "modalities : text\n",
            "\n",
            "available commands:\n",
            "  /exit or Ctrl+C     stop or exit\n",
            "  /regen              regenerate the last response\n",
            "  /clear              clear the chat history\n",
            "  /read               add a text file\n",
            "\n",
            "\u001b[1m\u001b[32m\n",
            ">     \n",
            "### Instruction:\n",
            "Translate the following English text into Arabic accurately.\n",
            "\n",
            "### Input:\n",
            "It was a soft purring voice ‐ a voice used deliberately as an instrument ‐ nothing impulsive\n",
            "or unpremeditated about it. Hercule Poirot swung round.\n",
            "He bowed. He shook hands ceremoniously.\n",
            "There was something in his eye that was unusual. One would have said that this chance\n",
            "encounter awakened in him an emotion that he seldom had occasion to feel.\n",
            "“My dear Mr. Shaitana,” he said.\n",
            "They both pause ... (truncated)\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b \bكان صوتًا هادئًا ومُعرَّضًا ‑ صوتًا تُستخدَمه عمدًا كأداة ‑ لم يكن فيه شيء غاضب أو غير متوقع. تحوَّل هيركول بوارو نحوه.\n",
            "رفرف. صافح بحفاوة. كان في عينيه شيء غير عادي. كان هذا اللقاء الذي حصل صدفةً ليُذكِّر هذا الشخص بمشاعرٍ نادرًا ما تكون له فرص لتجسيدها.\n",
            "\"أعزائي السيد شيتانا\" قال بوارو. توقفوا للحظة. كانا كأنهما متفرجين لمباراة. حولهما تجمهر من حشد لندن المترفّض. كان هناك بعض الأصوات التي تتدحرج أو تنثني.\n",
            "\u001b[35m\n",
            "[ Prompt: 3.0 t/s | Generation: 1.5 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> exit\n",
            "\u001b[0m\n",
            "|\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b\\\b|\b/\b-\b \bخروج\n",
            "\u001b[35m\n",
            "[ Prompt: 2.8 t/s | Generation: 2.3 t/s ]\n",
            "\u001b[0m\u001b[1m\u001b[32m\n",
            "> /\b/\b/exit\n",
            "\u001b[0m\n",
            "\n",
            "Exiting...\n",
            "llama_memory_breakdown_print: | memory breakdown [MiB] | total   free    self   model   context   compute    unaccounted |\n",
            "llama_memory_breakdown_print: |   - Host               |                 4284 =  3478 +     544 +     262                |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /usr/share/tesseract-ocr/tessdata/"
      ],
      "metadata": {
        "id": "QMvF48WH7v7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}